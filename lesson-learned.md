# What I learned

Disclaimer: the data used are generated by @chatgpt. 

## core concept

### embeddings
Un embedding est la transformation d'un texte en vecteur de nombres. Celui-ci est appelé un vecteur de dimension fixe (invariant). Sa position dans un espace mathématique permet de mesurer la similarité avec d'autres vecteurs. Par exemple pour le mot "médical" aura un vecteur proche de "médecin". 

Le modèle d'embedding place les textes dans un espace mathématique (eg euclidien) de grande dimension. Dans cet espace, on peut mesurer à quel point deux textes se ressemblent à l'aide du cosinus de similarité (et plus globalement via le produit scalaire), et correspond à cette notion de proximité sémantique.

Dans mon code, j'utilise des embeddings pour 1) les questions 2) le contexte stocké dans ma db de vecteurs. J'ai créé une fonction (principe DRY) pour cela : `get_embedding`. Elle prend le texte en entrée (question/contexte) et renvoie son vecteur d'embedding. 

#### a bit deeper - cosine similarity

Le cosinus de similarité est le cosinus de l'angle entre deux vecteurs. Ce résultat est compris entre [-1,1]. Si le produit est -1, les résultats sont opposés, 0 ils sont indépendants ou orthogonaux et 1 les résultats sont similaires. On dit qu'ils sont colinéaires [^1]. 

Le résultat du cosinus de deux angles s'obtient en prenant le produit scalaire divisé par le produit de leur norme.

$`\cos(\theta) = \dfrac{A \cdot B}{|A| , |B|}`$

> Produit scalaire : opération qui prend deux vecteurs et renvoie un nombre qui dépend à la fois de leur longueur et de l’angle entre eux. Ce nombre est d’autant plus grand que les vecteurs pointent dans la même direction.
> 
> $`A \cdot B`$

> Similarité cosinus : mesure normalisée de l’angle entre deux vecteurs. Plus l’angle est petit (vecteurs alignés), plus la similarité est proche de 1 ; plus l’angle est grand (vers 90°), plus la similarité tend vers 0. Et si les vecteurs pointent en sens strictement opposé, la similarité vaut -1. 

> Mesure normalisée : on ramène les valeurs à une échelle commune et bornée, par exemple entre -1 et 1, afin de pouvoir comparer des objets de tailles très différentes (par exemple entre un livre et un tweet) sur un pied d'égalité.
> 
> $`|A| , |B|`$
> On divise le produit scalaire par le produit des normes, ce qui revient à comparer uniquement les directions et pas les longueurs.

En définitive, le modèle d'embedding ne comprend pas le sens des mots comme un humain. Il manipule seulement des vecteurs et compare leurs positions dans l'espace. Si deux vecteurs sont proches, le modèle considère que les textes correspondants sont sémantiquement proche.

Dans le code, j'utilise un modèle d'embeddings. Il est entraîné pour associer un texte avec un vecteur fixe. Il joue le rôle de "traducteur". Je fais une particularité en plus dans mon code : je chunk le texte avant qu'il soit ingérer par le modèle d'embedding.

> based on: https://fr.wikipedia.org/wiki/Similarité_cosinus

[^1]: "colinéaire signifie sur une même ligne" (wikipédia - Similarité cosinus, 2025)

Pour le modèle d'embedding, le premier choix a été proposé par un outil (en l'occurrence Claude Code). Puis, différents modèles ont été demandés, au même outil, afin de mener des tests empiriques. 

Il n'y a pas eu de persistance des différents résultats et d'une évaluation approfondie, ces choix se sont faits par rapport : à la fenêtre de contexte, à la précision des réponses selon certaines questions arbitrairement choisies, et au taux de remplissage des questions. 

Modèle d'embedding à tester :
1. [ ] PubMedBERT 
2. [ ] BioLORD
3. [x] Nomic : text-embedding-nomic-embed-text-v1.5 (source: https://huggingface.co/nomic-ai/nomic-embed-text-v1.5)


### chunking

Le chunking consiste à découper un texte en plus petits morceaux (chunks). On peut le faire de plusieurs manières : par sémantique (content-aware - en respectant la structure et le sens), par phrase ou par paragraphe (simple sentence/paragraph splitting) ou encore par un nombre de caractères/tokens définis (fixed-size chunking).

Le chunking s'est avéré nécessaire pour optimiser le temps d'ingestion par le LLM (>10min sans découpage). J'ai choisi une approche par segments basés sur des blocs de texte de taille fixe, car c'était la plus simple à mettre en place et les résultats sont satisfaisants pour le moment.

Je découpe le texte tous les 2'000 caractères avec un overlap de 300. Le but de l'overlap est de garder le contexte précédent afin que le LLM garde une "vue" cohérente du texte, sans fournir un contexte trop grand (nuit aux performances).  

J'ai testé différent découpage de manière empirique, et cette configuration s'est révélée la plus satisfaisante pour l'instant.

Ce chunking est la piste d'amélioration la plus importante. Elle peut être améliorée en implémentant un chunking content-awre pour respecter les frontières logiques du texte. Il serait intéressant de mesurer si la qualité des réponses augmente et si l'impact sur les perforamnces (temps + coût) reste acceptable ou se dégrade légèrement. 

#### note for future me:

À des fins de review ou de modification, je décris les principes spécifiques que j'ai adapté à mon contexte.

Dans le code, j'ai centralisé cette logique dans la méthode `process_raw_data`.

Je fusionne tout le texte extrait (contexte) en une seule grande chaîne. Ensuite, je remplace les séquences qui sont > 3 par deux sauts de ligne. Je fais cela, car dans ma source (texte), les blocs logiques de texte sont déjà séparés par au moins trois sauts de ligne, et je veux normaliser cette séparation. 

Ensuite, je définis une zone de recherche locale : `search_zone = full_text[end - 100: end]`. C'est une petite fenêtre locale de 100 caractères avant la coupure théorique. L'objectif est d'éviter de couper une phrase/paragraphe en plein milieu. Je regarde avant la limite, si je peux trouver un endroit plus "propre" pour couper. 

Avec`re.finditer(r'\n\n', search_zone)`, je cherche un double saut de ligne dans cette fenêtre, ce qui correspond à une fin de paragraphe dans ma source. Si j'en trouve, je prends la dernière occurrence et je calcule un nouvel `end` avec : `end = (end - 100) + match[-1].end()`. Autrement dit, je décale la position de coupure juste après ce séparateur de paragraphe pour obtenir un chunk qui se termine proprement.

Si aucun double saut de ligne n'est trouvé, j'utilise un second regex : `re.finditer(r'(?<=[.?!])\s', search_zone)` pour chercher une fin de phrase. Elle cherche : `.`, `?` ou `!` suivi d'un espace (`\s`). Là encore, je me base sur la dernière occurrence trouvée pour ajuster la position de coupure au niveau d'une fin de prhase plutôt qu'en plein milieu.  

Une fois la position de coupure définitive choisie, j'extrais le chunk. Pour préparer le chunk suivant, je recule volontairement de 300 caractères par rapport à cette position. Cela crée l'overlap entre les chunks et garantit qu'une même phrase/idée n'est pas coupée trop brutalement entre deux morceaux. 

> based on: https://www.pinecone.io/learn/chunking-strategies/ 

### cross-encoder / reranking

Le reranking est une étape de raffinage qui intervient après la recherche initiale (retrieval). Son rôle est de réorganiser les documents récupérés pour ne conserver que les plus pertinents. Il agit comme un second filtre plus précis (d'un point de vue sémantique) et optimise la qualité du contexte envoyé au LLM. 

Il existe des méthodes classiques de reranking, comme la similarité cosinus (basée sur les vecteurs) ou le TF-IDF (basé sur la fréquence des mots). Cependant, les méthodes modernes utilisent des Cross-Encoders. Contrairement aux embeddings classiques qui traitenet la question et le document séparément, le Cross-Encoder analyse la question et le document simultanément pour en saisir toutes les interactions sémantiques. 

Le fonctionnement d'un Cross-Encoder repose sur l'évaluation de paires : il prend en entrée le couple (eg question, document) et lui attribue un score de pertinence entre 0 et 1. Cette analyse jointe epremt de détecter des nuances que les embeddings seuls pourraient manquer, garantissant que le document choisi est réellemtn une réponse à la question posée. 

Le principal inconvénient est le coût computationnel élevé : analyser chaque paire est beaucoup plus lent que de comparer des vecteurs pré-calculés. Une stratégie d'optimisation consiste à utiliser le reranking de manière sélective. Pour des recherches factuelles (eg nom/prénom), la similarité cosinus est suffisante. Pour des analyses complexes (eg anamnèse d'un patient), la puissance du Cross-Encoder devient indispensable pour garantir l'exactitude médicale. 

> based on: https://blent.ai/blog/a/reranking-tout-savoir

Le modèle `cross-encoder/ms-marco-MiniLM-L-6-v2` que j'utilise est un bon compromis. Il est basé sur une architecture BERT, ce qui le rend rapide tout en conservant d'excellentes performances pour le reranking, car il a été entraîné spécifiquement sur le datasset MS MARCO (conçu pour la recherche d'information réelle).

> based on: https://huggingface.co/cross-encoder/ms-marco-MiniLM-L6-v2

### batches

- un chunk = segment de texte issu du contexte source. Dans le code, c'est généré par `process_raw_data()`avec une limite (`CHUNK_SIZE`) et un overlap (`CHUNK_OVERLAP`). Ainsi, chaque chunk est un morceau du document. 
- un batch est un groupe de questions ou de champs (et pas du texte) traités ensemble par le LLM. Dans le code : `batches = list(chunk_list(valide_fields, BATCH_SIZE))`. Par exemple, chaque batch contient 5 questions maximums (`BATCH_SIZE`). 

Un batch a plusieurs avantages. Il limite la taille du prompt, dans le code : chaque batch construit un context (`context_str`et envoie un prompt avec plusieurs questions (issus de la template). Si la template contient trop de question, le prompt grossirait (question + contexte + instruction = beaucoup), et la fenêtre de contexte serait dépasser. Ainsi, cela permet d'éviter ce problème. Le second avantage, est le reranking. Le reranker travaille avec `combined_questions` (soit toutes les questions du batch). Il trie les documents (contexte) utile pour l'ensemble des questions. Si le batch est trop gros, il y a un mélange (bruit) et la précision baisse. Et finalement, cela permet de contrôler le coût et la latence. Un batch trop gros signifie plus d'appels, c'est long et ça coûte cher. Et le risque d'échec est plus grand. Des petits batchs permettent de pallier ce problème (plus d'appels, mais plus fiables et faciles à diagnostiquer). 

---

# Tools that I used
@perplexity & @chatgpt pour corriger certains passages et utiliser les bons termes techniques. Ils ont également servi pour générer de fausses données d'exemples sur lesquels une partie de ce PoC se base. 
@claude lancer le PoC (syndrome page blanche) et pour améliorer certaines méthodes et m'aider dans le chunking
@antidote pour l'orthographe et la reformulation
